{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJfo68vrkh1n",
        "outputId": "419d9e1f-691c-40eb-b0f7-41ddeecfac7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-goa2qhuo\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-goa2qhuo\n",
            "  Resolved https://github.com/openai/whisper.git to commit dd985ac4b90cafeef8712f2998d62c59c3e62d22\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803707 sha256=13b4a2d9339dc6300aa188399caf803a34641fbb51a61d6721df50f3622e43f0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vf9_o180/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,748 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,532 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,053 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,253 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,024 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,703 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,561 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,340 kB]\n",
            "Fetched 30.6 MB in 3s (10.8 MB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "37 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"2025-06-17 19-03-30.mkv\" --model large # Replace the file name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAbubv72lTrH",
        "outputId": "17018386-7ebf-4309-9c3e-f959edb45b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|█████████████████████████████████████| 2.88G/2.88G [00:51<00:00, 60.2MiB/s]\n",
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: Greek\n",
            "[00:00.000 --> 00:11.000]  Υπάρχουν δύο-τρία classification μοντέλα και μπορείτε να το δρομολογήσετε σαν ένα μοντέλο το οποίο θα έχει multiple outputs.\n",
            "[00:11.000 --> 00:15.000]  Δηλαδή το input θα είναι ένα και το output θα είναι τρία.\n",
            "[00:15.000 --> 00:23.000]  Θα κάνουμε multilabel classification στις κατηγορίες, binary classification στις sentiments μόνος\n",
            "[00:23.000 --> 00:31.000]  και το άλλο τι είπατε ότι ήταν, sentiment, κατηγορίες είπατε και το urgency.\n",
            "[00:31.000 --> 00:37.000]  Ναι, αλλά το urgency, πρώτα απ' όλα, δεδομένα για αυτά έχετε.\n",
            "[00:46.000 --> 00:48.000]  Και από το κανγκλ είχαμε βρει κάποιοι.\n",
            "[00:53.000 --> 00:54.000]  Α, όχι.\n",
            "[01:04.000 --> 01:06.000]  Αν είναι μόνο συνθετικά...\n",
            "[01:16.000 --> 01:19.000]  Ε, ναι, γιατί δεν είναι real data, επομένως.\n",
            "[01:23.000 --> 01:26.000]  Λίγο με προβληματίζει αυτό για το τι θα βγάλετε.\n",
            "[01:26.000 --> 01:31.000]  Αυτό για το από το κανγκλ πάντως επειδή το έψαξα είναι όντως, είναι όλα συνθετικά.\n",
            "[01:31.000 --> 01:33.000]  Απλά αυτός, αυτό που είχε κάνει...\n",
            "[01:33.000 --> 01:36.000]  Λίγο δεν είναι συνθετικά και είναι anonymized, είναι άλλο...\n",
            "[01:36.000 --> 01:43.000]  Όχι, όχι, όχι, αυτό, ένα που έχω βρει από το κανγκλ είναι καθαρά συνθετικά.\n",
            "[01:43.000 --> 01:46.000]  Απλά είχε χρησιμοποιήσει προς κάποιους λογιστές μέσα από το chasgpt\n",
            "[01:46.000 --> 01:52.000]  και είχε φτιάξει σαν agents θεωρητικά και κάθε agent είχε ένα task, έπαιρνε αυτό.\n",
            "[01:53.000 --> 01:57.000]  Ας πούμε, ο ένας έφτιαχνε την ερώτηση, ο άλλος έφτιαχνε την απάντηση,\n",
            "[01:57.000 --> 01:59.000]  ο άλλος το κατηγορηποιούσε.\n",
            "[01:59.000 --> 02:03.000]  Το είχε σπάσει έτσι, δηλαδή δεν ήταν ένα από τα δασόλια.\n",
            "[02:03.000 --> 02:22.000]  Αυτό το data set είχε νομίζω γύρω στα 20.000.\n",
            "[02:23.000 --> 02:28.000]  Okay.\n",
            "[02:28.000 --> 02:35.000]  Okay.\n",
            "[02:35.000 --> 02:46.000]  Έχει και κάποια tags εδώ, βλέπω.\n",
            "[02:46.000 --> 02:49.000]  Και αυτό είναι ένα ωραίο.\n",
            "[02:49.000 --> 02:51.000]  Να κάνετε automated tags.\n",
            "[02:51.000 --> 02:55.000]  και κάνετε automated tagging στο κείμενο.\n",
            "[02:55.000 --> 02:58.000]  Επίσης, έχει και την απάντηση...\n",
            "[02:58.000 --> 03:00.000]  Επειδή είστε τέσσερις, θα είμαι ειλικρινής μαζί σας,\n",
            "[03:00.000 --> 03:06.000]  επειδή είστε τέσσερις, το να μου κάνετε ένα classification σε αυτά τα τρία είναι λίγο.\n",
            "[03:06.000 --> 03:10.000]  Αμέσως, στον δύο άτομα θα ήταν more than enough, αλλά είστε τέσσερις.\n",
            "[03:10.000 --> 03:14.000]  Επομένως, αν μου πείτε ότι θα τα κάνετε και τα τέσσερα, δηλαδή,\n",
            "[03:14.000 --> 03:18.000]  classification σε... πώς το λένε...\n",
            "[03:18.000 --> 03:23.000]  Αν μου χτίσετε, δηλαδή, ξεχωριστά μοντέλα ως προς τα τέσσερα πράγματα που είπαμε\n",
            "[03:23.000 --> 03:30.000]  και μου χτίσετε και ένα μοντέλο το οποίο θα παίρνει ένα input και θα βγάζει όλα τα outputs,\n",
            "[03:30.000 --> 03:33.000]  εγώ θα είμαι ευχαριστημένος.\n",
            "[03:33.000 --> 03:35.000]  Οκ.\n",
            "[03:35.000 --> 03:40.000]  Και αν θέλετε να πάτε, και επειδή είστε τέσσερις, να το πάτε και σε ένα next step,\n",
            "[03:40.000 --> 03:43.000]  θα μπορούσατε να χρησιμοποιήσετε, επίσης, το Streamlit\n",
            "[03:43.000 --> 03:45.000]  και, προφανώς, να χρησιμοποιήσετε ένα LLM\n",
            "[03:45.000 --> 03:48.000]  που θα σας βοηθήσει να χτίσετε και ένα dummy UI\n",
            "[03:48.000 --> 03:51.000]  το οποίο θα βάζει σε ένα κείμενο\n",
            "[03:51.000 --> 03:56.000]  και θα σας βγάζει όλα τα predictions για κάθε ένα από τα... πώς το λένε...\n",
            "[03:56.000 --> 03:59.000]  πράγματα που θέλετε.\n",
            "[03:59.000 --> 04:02.000]  Και αν θέλετε να το πάτε και ένα step further,\n",
            "[04:02.000 --> 04:06.000]  θα μπορούσατε να το κάνετε incorporate με κάποιο MCP.\n",
            "[04:06.000 --> 04:09.000]  Σας έχουν πει τι είναι το MCP.\n",
            "[04:09.000 --> 04:11.000]  Ναι.\n",
            "[04:11.000 --> 04:13.000]  Να χρησιμοποιήσετε, επίσης, με το ClickUp.\n",
            "[04:13.000 --> 04:15.000]  Να φτιάξετε ένα account στο ClickUp.\n",
            "[04:15.000 --> 04:17.000]  Οκ.\n",
            "[04:17.000 --> 04:22.000]  Και αφού μπαίνει το task και γίνεται αυτή η κατηγοριοποίηση ή οτιδήποτε,\n",
            "[04:22.000 --> 04:27.000]  να χρησιμοποιήσετε το MCP του ClickUp και να πηγαίνει να φτιάχνει αυτόματα πάνω...\n",
            "[04:27.000 --> 04:29.000]  πώς τα λένε...\n",
            "[04:29.000 --> 04:33.000]  να φτιάχνει αυτόματα πάνω tasks\n",
            "[04:33.000 --> 04:39.000]  τα οποία θα πηγαίνει να βάζει, επίσης, τα tags, τα οποία θα έχετε κάνει εσείς predict,\n",
            "[04:39.000 --> 04:42.000]  θα βάζει ένα sentiment, θα βάζει ένα description,\n",
            "[04:42.000 --> 04:45.000]  το οποίο θα είναι το description που θα έχετε βάλει,\n",
            "[04:45.000 --> 04:48.000]  αλλά θα βάζει και τα labels αυτά.\n",
            "[04:48.000 --> 04:50.000]  Οκ.\n",
            "[04:50.000 --> 04:56.000]  Νομίζω ότι είναι, αν το πάτε end-to-end, θα είναι και πάρα πολύ ωραίο ο use case να το δείξετε παρά έξω μετά.\n",
            "[04:56.000 --> 04:59.000]  Λογικά είστε part-timers, δεν είστε.\n",
            "[04:59.000 --> 05:01.000]  Να.\n",
            "[05:01.000 --> 05:04.000]  Δηλαδή, εγώ, ας πούμε, το MCP του ClickUp το χρησιμοποιώ κατά core.\n",
            "[05:04.000 --> 05:06.000]  Το έχω συνδέσει με το cursor.\n",
            "[05:06.000 --> 05:08.000]  Ξέρετε το cursor, αν γίνει.\n",
            "[05:09.000 --> 05:20.000]  Έχω βάλει στο cursor το, καλά και στο chat GPT μπορεί να το βάλεις πια.\n",
            "[05:20.000 --> 05:24.000]  Αλλά ουσιαστικά αυτό που μπορείτε να κάνετε είναι,\n",
            "[05:24.000 --> 05:32.000]  είτε να πείτε ότι μένω στο ότι θα παίρνω ένα input και θα κάνω predict τα τρία-τέσσερα διαφορετικά,\n",
            "[05:32.000 --> 05:36.000]  τις τρεις-τέσσερις διαφορετικούς στόχους και μένω εκεί,\n",
            "[05:36.000 --> 05:39.000]  είτε μπορείτε να φτιάξετε ένα,\n",
            "[05:39.000 --> 05:41.000]  ένα agent ο οποίος θα παίρνει αυτό,\n",
            "[05:41.000 --> 05:43.000]  θα χρησιμοποιήσει tools,\n",
            "[05:43.000 --> 05:47.000]  αυτά τα δύο-τρία μοντέλα που θα έχετε προεκπαιδεύσει\n",
            "[05:47.000 --> 05:49.000]  και θα τα γυρνάει επίσης σε μια,\n",
            "[05:49.000 --> 05:50.000]  πώς το λένε,\n",
            "[05:50.000 --> 05:51.000]  έντυπη,\n",
            "[05:51.000 --> 05:52.000]  ας πούμε,\n",
            "[05:52.000 --> 05:53.000]  πώς το λένε.\n",
            "[05:53.000 --> 05:54.000]  Γιατί τώρα αυτό,\n",
            "[05:54.000 --> 05:56.000]  ένα LLM μπορεί να το κάνει και από μόνο του.\n",
            "[05:56.000 --> 06:00.000]  Βάζει ένα κείμενο, του λες βγάλε μου αυτά, βγάλε μου αυτά, βγάλε μου αυτά και να το κάνει από μόνο του.\n",
            "[06:00.000 --> 06:02.000]  Αλλά πολλές φορές αυτό είναι πάρα πολύ ακριβό,\n",
            "[06:02.000 --> 06:04.000]  γενικά, όταν να το κάνεις.\n",
            "[06:04.000 --> 06:05.000]  Έτσι.\n",
            "[06:05.000 --> 06:08.000]  Επομένως αυτό που μπορεί να κάνεις είναι να, θα το δούμε και στην τάξη,\n",
            "[06:08.000 --> 06:11.000]  να ενώσετε tools πάνω σε έναν agent.\n",
            "[06:11.000 --> 06:12.000]  Οκ.\n",
            "[06:12.000 --> 06:15.000]  Οπότε ένα, ένα tool θα είναι ένα προεκπαιδευμένο μοντέλο\n",
            "[06:15.000 --> 06:19.000]  που θα κάνει sentiment analysis.\n",
            "[06:19.000 --> 06:22.000]  Ένα άλλο προεκπαιδευμένο μοντέλο που θα κάνει\n",
            "[06:22.000 --> 06:24.000]  το τι τύπος είναι.\n",
            "[06:24.000 --> 06:27.000]  Ένα άλλο προεκπαιδευμένο μοντέλο το οποίο θα λέει\n",
            "[06:27.000 --> 06:30.000]  ποια είναι τα tags.\n",
            "[06:30.000 --> 06:31.000]  Οκ.\n",
            "[06:31.000 --> 06:32.000]  Οπότε,\n",
            "[06:32.000 --> 06:34.000]  ο agent θα καλεί αυτά τα τρία,\n",
            "[06:34.000 --> 06:36.000]  αυτά από πίσω θα τρέχουν το inference,\n",
            "[06:36.000 --> 06:38.000]  θα επιστρέφουν τα αποτελέσματα,\n",
            "[06:38.000 --> 06:42.000]  μετά θα τα παίρνει ο agent και θα γυρνάει πίσω μια απάντηση.\n",
            "[06:42.000 --> 06:44.000]  Αυτό.\n",
            "[06:44.000 --> 06:46.000]  Έτσι.\n",
            "[06:46.000 --> 06:48.000]  Ναι.\n",
            "[06:48.000 --> 06:52.000]  Άρα ο agent θα είναι συνδεμένο με κάποιο LLM.\n",
            "[06:52.000 --> 06:53.000]  Ναι.\n",
            "[06:53.000 --> 06:54.000]  Ναι.\n",
            "[06:54.000 --> 06:55.000]  Οκ.\n",
            "[06:55.000 --> 06:57.000]  Και θα καλεί κάποια tools από πίσω, μπορεί να είναι,\n",
            "[06:57.000 --> 07:00.000]  πώς το λένε, κάποιο open source LLM.\n",
            "[07:00.000 --> 07:03.000]  Δεν είναι απαραίτητα να είναι το GPT ή κάποιο άλλο.\n",
            "[07:03.000 --> 07:04.000]  Έτσι.\n",
            "[07:04.000 --> 07:09.000]  Ξέρει να χρησιμοποιεί το LLM από πίσω tools.\n",
            "[07:09.000 --> 07:11.000]  Εσείς θα έχετε φτιάξει ουσιαστικά τα tools\n",
            "[07:11.000 --> 07:13.000]  τα οποία θα είναι προεκπαιδευμένα μοντέλα\n",
            "[07:13.000 --> 07:15.000]  πάνω στα tasks που θέλετε.\n",
            "[07:15.000 --> 07:18.000]  Θα παίρνει τις απαντήσεις πίσω από τα inferences\n",
            "[07:18.000 --> 07:21.000]  που θα έχουν γίνει μέσω των μοντέλων\n",
            "[07:21.000 --> 07:23.000]  και θα ξέρει να τα βγάλει και μετά ενδεχομένως\n",
            "[07:23.000 --> 07:26.000]  να καλέσει με MCP το ClickUp\n",
            "[07:26.000 --> 07:29.000]  και να πάει να φτιάξει το task πάνω\n",
            "[07:29.000 --> 07:32.000]  και να του βάλει τα tags, να βγάλει ένα sentiment,\n",
            "[07:32.000 --> 07:33.000]  ένα urgency κτλ. κτλ.\n",
            "[07:33.000 --> 07:33.940]  και τα λοιπά και τα λοιπά.\n",
            "[07:34.500 --> 07:36.840]  Απλά αυτό που θα πρέπει να κάνετε είναι να πάτε στο ClickUp πρώτα,\n",
            "[07:36.940 --> 07:38.240]  να φτιάξετε ένα ClickUp account.\n",
            "[07:38.880 --> 07:38.980]  Οκ.\n",
            "[07:40.360 --> 07:42.080]  Μισό να σας δείξω κιόλας.\n",
            "[07:42.420 --> 07:44.660]  Νομίζω θα το λέω έτσι, πώς το λένε.\n",
            "[07:46.820 --> 07:48.080]  Screen 1.\n",
            "[07:48.680 --> 07:49.020]  Share.\n",
            "[07:49.940 --> 07:52.040]  Αυτό σας το δείχνω, νομίζω, και στην τάξη.\n",
            "[07:55.520 --> 07:55.960]  Έλα.\n",
            "[07:55.960 --> 07:56.120]  Ωραία.\n",
            "[07:59.180 --> 07:59.620]  Ωραία.\n",
            "[08:03.000 --> 08:06.820]  Ναι, ASANA, ναι, ναι, ναι, και όλα αυτά.\n",
            "[08:06.900 --> 08:08.900]  Απλά δεν ξέρω αν το ASANA έχει βγάλει MCP.\n",
            "[08:09.380 --> 08:10.000]  Θα σου πω αμέσως.\n",
            "[08:16.920 --> 08:17.400]  Έχει.\n",
            "[08:20.220 --> 08:20.700]  Όχι.\n",
            "[08:21.220 --> 08:21.920]  Όχι, όχι.\n",
            "[08:21.980 --> 08:24.180]  Το Streamlit είναι Python Package,\n",
            "[08:24.180 --> 08:27.560]  το οποίο σε βοηθάει να γράψεις πάρα πολύ γρήγορα UI interfaces.\n",
            "[08:27.980 --> 08:28.080]  Οκ.\n",
            "[08:29.580 --> 08:32.420]  Αλλά αν πάμε εδώ στο που χρησιμοποιώ,\n",
            "[08:32.540 --> 08:32.980]  βλέπετε,\n",
            "[08:33.000 --> 08:34.560]  αυτά εδώ που έχουν οι κομιδιάκια.\n",
            "[08:35.120 --> 08:36.820]  Ναι, ναι, να το είχαμε δει αυτό, ναι.\n",
            "[08:37.980 --> 08:39.700]  Αυτά εδώ μου τα έχει γράψει μόνο του.\n",
            "[08:41.920 --> 08:42.160]  Οκ.\n",
            "[08:46.260 --> 08:46.700]  Βλέπετε.\n",
            "[08:47.280 --> 08:47.580]  Ναι.\n",
            "[08:47.800 --> 08:49.440]  Μπορούσε να σου βάλει και τα tags.\n",
            "[08:50.480 --> 08:53.580]  Εδώ, εσείς θα έχετε κάνει inference, θα έχετε βγάλει τα tags,\n",
            "[08:53.900 --> 08:56.200]  και θα του λες, πήγαινε να βάλει και τα tags στο ticket.\n",
            "[08:58.720 --> 09:01.020]  Οπότε, αυτοματοποιείς μια διαδικασία,\n",
            "[09:01.020 --> 09:02.740]  που κάποιος από ένα τέτοιο,\n",
            "[09:02.740 --> 09:04.740]  συζητάει το ticket.\n",
            "[09:04.740 --> 09:08.740]  Το ticket, ο agent το παίρνει αυτό,\n",
            "[09:08.740 --> 09:11.740]  καλεί τα τρία tools για να πάρει τα τρία inferences,\n",
            "[09:11.740 --> 09:15.740]  έχετε προεκπαιδεύσει στα μοντέλα για αυτό,\n",
            "[09:15.740 --> 09:18.740]  μη σας φοβίζω, αλλά αυτό είναι πάρα πολύ ρεπλό να το κάνετε, έτσι.\n",
            "[09:18.740 --> 09:20.740]  Είναι αστείο να το κάνετε ψα.\n",
            "[09:20.740 --> 09:24.740]  Πριν πέντε χρόνια, θα ήθελε μια ομάδα από ML engineers.\n",
            "[09:24.740 --> 09:26.740]  Πλέον, είναι αστείο.\n",
            "[09:26.740 --> 09:28.740]  Οκ.\n",
            "[09:28.740 --> 09:30.740]  Επομένως,\n",
            "[09:30.740 --> 09:31.940]  τα καλή, τα tools,\n",
            "[09:31.940 --> 09:38.940]  το οποίο θα λέει είναι τάδε sentiment, είναι τάδε priority, είναι σε αυτές τις κατηγορίες,\n",
            "[09:38.940 --> 09:40.940]  και είναι και αυτά τα tags,\n",
            "[09:40.940 --> 09:42.940]  θα τα παίρνει το llm,\n",
            "[09:42.940 --> 09:44.940]  θα καλεί μετά το click up,\n",
            "[09:44.940 --> 09:46.940]  και θα ανεβάζει το task, εκεί που πρέπει.\n",
            "[09:48.940 --> 09:50.940]  Το τελευταίο είναι optional.\n",
            "[09:50.940 --> 09:52.940]  Ξεκινήστε με το κομμάτι,\n",
            "[09:52.940 --> 09:54.940]  και είναι και μάλιστα και το πιο εύκολο.\n",
            "[09:54.940 --> 09:58.940]  Το να κουμπώ σε ένα mcp, σε ένα llm, είναι αστείο.\n",
            "[09:58.940 --> 10:01.940]  Οκ, αν το ψάξετε, υπάρχουν έτοιμα πράγματα.\n",
            "[10:01.940 --> 10:03.940]  Δηλαδή, εγώ για να το κουμπώσω στο cursor,\n",
            "[10:03.940 --> 10:05.940]  μισό λεπτό να σας το δείξω.\n",
            "[10:05.940 --> 10:07.940]  Για να το κουμπώσω στο cursor, έτσι,\n",
            "[10:07.940 --> 10:10.940]  για να το κουμπώσω σε ένα γενικότερο AI agent,\n",
            "[10:10.940 --> 10:13.940]  αλλά και αυτό είναι εξίσου απλό, πλέον.\n",
            "[10:16.940 --> 10:20.940]  Show settings, cursor settings,\n",
            "[10:20.940 --> 10:22.940]  punto.\n",
            "[10:26.940 --> 10:28.940]  Να το.\n",
            "[10:28.940 --> 10:30.940]  Οκ.\n",
            "[10:30.940 --> 10:32.940]  Βλέπετε εδώ. Αυτό είναι.\n",
            "[10:34.940 --> 10:36.940]  Αυτό.\n",
            "[10:39.940 --> 10:41.940]  Τίποτα άλλο.\n",
            "[11:00.940 --> 11:12.940]  Go fancy creating.\n",
            "[11:12.940 --> 11:14.940]  Ή μ SURFACE.\n",
            "[11:14.940 --> 11:16.940]  Ωραία.\n",
            "[11:16.940 --> 11:18.940]  Ωραία.\n",
            "[11:18.940 --> 11:21.940]  Ωραία.\n",
            "[11:21.940 --> 11:24.940]  Ωραία.\n",
            "[11:24.940 --> 11:26.940]  Ωραία.\n",
            "[11:26.940 --> 11:28.940]  Ωραία.\n",
            "[11:28.940 --> 11:39.040]  Ο ΛΑΜΑ παρέχει ουσιαστικά μια πλατφόρμα για να κατεβάζεις τοπικά μοντέλα και να τρώχεις τοπικά.\n",
            "[11:40.480 --> 11:45.980]  Εμείς όταν θα χτίσουμε AI agents, του λέμε από πού θέλουμε να παίρνει την πληροφορία των agents.\n",
            "[11:46.600 --> 11:53.300]  Είτε του παίρνεις κάποιο API key και χτυπηπάς πηχή OpenAI, είτε του λες συνδέσουμε ο ΛΑΜΑ και χρησιμοποιήσε το μοντέλο τάδε.\n",
            "[11:53.300 --> 12:12.940]  Οπότε μπορείτε να παίξετε με TPC Car1, μπορείτε να παίξετε με QN, μπορείτε να παίξετε με το CRI-CRI που λέγατε, που σας έλεγε προχθές ο Σουκράτης και τα λοιπά.\n",
            "[12:15.340 --> 12:19.520]  Επομένως, αυτό το μάθημα θα το κάνουμε την επόμενη Παρασκευή.\n",
            "[12:21.300 --> 12:23.280]  Μεθαύριο θα δούμε λίγο Computer Vision.\n",
            "[12:23.300 --> 12:25.720]  Και θα δούμε Fine Tuning σε LLMs.\n",
            "[12:26.420 --> 12:32.520]  Αλλά το τελευταίο μάθημα που θα έχετε μαζί μου, δηλαδή και αυτή η Παρασκευή και την επόμενη θα έχουμε μαζί μάθημα.\n",
            "[12:33.140 --> 12:36.460]  Και με τα λογικά 4 Ιουλίου θα έχετε και άλλο ένα τελευταίο με το Χάρη.\n",
            "[12:38.560 --> 12:47.480]  Θα τα δείτε. Θα δούμε πώς φτιάχνουμε agent systems, agentic systems και multi-agentic systems που χρησιμοποιούν tools.\n",
            "[12:47.980 --> 12:51.180]  Επομένως, εσείς απλά τι θα κάνετε, θα αλλάξετε τα tools.\n",
            "[12:51.340 --> 12:53.280]  Και τι θα κάνετε tool, ένα infernal.\n",
            "[12:53.300 --> 12:54.140]  Και τι θα κάνετε από ένα μοντέλο.\n",
            "[13:07.640 --> 13:09.500]  Εξαρτάται το task που έχεις να λύσεις.\n",
            "[13:11.300 --> 13:15.560]  Πρέπει να έχεις κάποιο μοντέλο, εξαρτάται σε τι γλώσσα θα παίξεις, τα γλυκά υποθέτω.\n",
            "[13:16.600 --> 13:18.120]  Υπάρχουν άπειρα μοντέλα.\n",
            "[13:18.120 --> 13:18.660]  Έτσι.\n",
            "[13:20.420 --> 13:21.540]  Τώρα.\n",
            "[13:23.300 --> 13:32.180]  Ανάλογα το task, θα πάτε στο hacking face, θα βρείτε tasks, θα ψάξετε λίγο και θα δείτε ποιο task είναι πιο κοντά σε αυτό που θέλει να φτιάξετε.\n",
            "[13:32.880 --> 13:39.740]  Αλλά εγώ θα σας πρότεινα να δημιουργήσετε και ένα χέρας μοντέλο, το οποίο να παίρνει ένα input και να βγάζει και τα τέσσερα outputs.\n",
            "[13:40.180 --> 13:49.320]  Χρησιμοποιήστε, π.χ., το chat GPT, να σας βοηθήσει να φτιάξετε ένα μοντέλο το οποίο θα έχει ένα input και θα προσπαθεί να κάνει τρία-τέσσερα inferences.\n",
            "[13:50.020 --> 13:51.100]  Απλά θα πρέπει να το πείτε.\n",
            "[13:51.100 --> 13:55.840]  Το πρώτο θα είναι τάδε, που θα είναι οι κατηγορίες και θα είναι multi-label ή multi-class.\n",
            "[13:55.980 --> 13:57.380]  Θα ανήκει σε μία κατηγορία ή πολλές.\n",
            "[13:59.100 --> 14:04.060]  Πρέπει να το αποφασίσετε και να του πείτε ότι αν θα έχει πολλές κατηγορίες, θα είναι multi-label.\n",
            "[14:04.260 --> 14:07.100]  Αν θα είναι μία κατηγορία, θα είναι multi-class.\n",
            "[14:08.280 --> 14:11.600]  Το sentiment θα είναι binary classification.\n",
            "[14:12.320 --> 14:14.180]  Τα tags είναι κάποιο άλλο τύπου.\n",
            "[14:14.300 --> 14:16.920]  Θα πρέπει να σας το αφήσω για άσκηση, λίγο να το ψάξετε.\n",
            "[14:18.680 --> 14:20.920]  Και άλλο ένα μου είπατε.\n",
            "[14:21.100 --> 14:23.100]  Τι άλλο μου είπατε.\n",
            "[14:23.860 --> 14:24.600]  Και το urgency.\n",
            "[14:25.060 --> 14:29.100]  Αν είναι urgent, non-urgent, είναι πάλι binary classification.\n",
            "[14:30.940 --> 14:36.800]  Επομένως, αυτό που θα κάνετε είναι να πείτε ότι θέλω να έχω one input και τέσσερα heads.\n",
            "[14:37.060 --> 14:41.220]  Το οποίο θα κάνει τέσσερα διαφορετικά classifications at the same time.\n",
            "[14:43.680 --> 14:45.760]  Αλλά αυτό θα είναι ένα έξτρα μοντέλο.\n",
            "[14:46.720 --> 14:49.100]  Δηλαδή θέλετε τέσσερα που να κάνουν το καθένα.\n",
            "[14:49.100 --> 14:50.940]  Μπορείτε να κάνετε τέσσερα ξεχωριστά.\n",
            "[14:51.100 --> 14:53.340]  Και οφείλετε να κάνετε τέσσερα ξεχωριστά.\n",
            "[14:53.760 --> 14:57.400]  Και οφείλετε να κάνετε και ένα που μπορεί να τα κάνει και τα τέσσερα μαζί.\n",
            "[14:57.820 --> 14:59.040]  Και να δείτε ποιο παίζει καλύτερα.\n",
            "[14:59.140 --> 15:03.800]  Γιατί πολλές φορές, όταν εκπαιδεύετε ένα μοντέλο σε διαφορετικά outputs ταυτόχρονα,\n",
            "[15:04.500 --> 15:05.920]  μαθαίνει και περισσότερα πράγματα.\n",
            "[15:09.380 --> 15:13.740]  Μη σας φοβίζει πλέον, παιδιά, το τσάντζι επειδή κάνει θαύματα,\n",
            "[15:13.960 --> 15:16.880]  θα σας φανεί πάρα πάρα πολύ χρήσιμο.\n",
            "[15:16.880 --> 15:19.800]  Απλά πείτε το να χρησιμοποιήσετε hacking face.\n",
            "[15:20.480 --> 15:20.880]  Για...\n",
            "[15:21.100 --> 15:22.440]  Τα μεμονωμένα μοντέλα.\n",
            "[15:22.940 --> 15:27.200]  Γιατί δεν ξέρω αν υπάρχουν, μάλιστα, μισό, πάμε να το δούμε μαζί βασικά.\n",
            "[15:31.180 --> 15:32.480]  Hacking face.\n",
            "[15:33.880 --> 15:34.820]  Βλέπετε την οθόνη μου.\n",
            "[15:35.400 --> 15:35.680]  Να.\n",
            "[15:37.180 --> 15:40.480]  Text generation, image to text, any to any.\n",
            "[15:41.160 --> 15:42.360]  Τι είναι αυτό το any to any.\n",
            "[15:46.360 --> 15:46.880]  Μισό.\n",
            "[15:47.120 --> 15:47.880]  Εκόμα αγιώς.\n",
            "[15:51.100 --> 15:53.100]  Τάσκης.\n",
            "[15:53.100 --> 15:55.100]  Εδώ είμαστε.\n",
            "[15:55.100 --> 16:03.100]  Computer vision, text classification, βλέπετε εδώ.\n",
            "[16:03.100 --> 16:09.100]  Ένα είναι αυτό που θα ασχοληθείτε, οπότε πατάτε εδώ και βλέπετε όλα αυτά.\n",
            "[16:09.100 --> 16:13.100]  Βλέπετε institutional books topic classifier.\n",
            "[16:13.100 --> 16:17.100]  Οπότε μπορείτε να χρησιμοποιήσετε, αν γράψετε εδώ τη λέξη topic,\n",
            "[16:17.100 --> 16:19.100]  σου βγάζει αυτό και σου λέει Ινδονήσια.\n",
            "[16:19.100 --> 16:20.100]  Ωραία.\n",
            "[16:20.100 --> 16:21.060]  Ωραία.\n",
            "[16:21.060 --> 16:22.060]  Ωραία.\n",
            "[16:22.060 --> 16:27.060]  Ψάχτε λίγο τα μοντέλα εδώ για να δείτε ποιο μπορείτε να χρησιμοποιήσετε.\n",
            "[16:27.060 --> 16:28.060]  Έτσι.\n",
            "[16:28.060 --> 16:29.060]  Αγιώς.\n",
            "[16:29.060 --> 16:34.060]  Θέλετε, μπορείτε να δοκιμάσετε και zero-shot classification.\n",
            "[16:34.060 --> 16:35.060]  Οκ.\n",
            "[16:35.060 --> 16:44.060]  Το zero-shot το περνάς δυναμικά labels, το έχουμε κάνει στην τάξη και προσπαθεί να σου βγάλει επί τόπου ποιες είναι οι κατηγορίες.\n",
            "[16:44.060 --> 16:47.060]  Δηλαδή αν εσείς έχετε, πόσες κατηγορίες έχετε.\n",
            "[16:51.060 --> 16:57.060]  Ωραία.\n",
            "[16:57.060 --> 17:02.060]  Τα πιο δύο, τρεις, το πρώτο τασκ, που ήταν σε ποια ο topic ανήκει, πόσα topics είναι.\n",
            "[17:02.060 --> 17:11.060]  Αν είναι πάνω από δέκα δεν μπαίνετε με zero-shot, αλλά αν είναι με…\n",
            "[17:11.060 --> 17:15.060]  Το topic δεν το έχουμε αποφασιστώ, το urgency είναι high, medium, low ας πούμε.\n",
            "[17:15.060 --> 17:17.060]  Οκ.\n",
            "[17:17.060 --> 17:19.060]  Κοίτα, εκεί δεν θα μπορέσει να σου το κάνει.\n",
            "[17:19.060 --> 17:23.740]  Να καταλάβει το urgent δεν θα το καταλάβει, αλλά αν καταλάβει ποιο τοπικ είναι θα το καταλάβει.\n",
            "[17:25.500 --> 17:28.500]  Μέχρι πόσο είναι καλά για να το καταλάβει.\n",
            "[17:29.400 --> 17:31.180]  Όχι, είναι το θέμα τι θα καταλάβει.\n",
            "[17:31.280 --> 17:35.540]  Είναι πολύ καλό να καταλάβει σε ποιο τοπικ μπορεί να ανήκει ένα κείμενο.\n",
            "[17:35.540 --> 17:39.160]  Αλλά δεν νομίζω ότι μπορεί να καταλάβει πόσο urgent είναι ένα κείμενο.\n",
            "[17:43.580 --> 17:48.520]  Μετά, text, speech, tabular, reinforcement, other.\n",
            "[17:48.520 --> 17:49.640]  Εδώ έχει γράφους.\n",
            "[17:51.040 --> 17:54.920]  Δεν βλέπω κάτι εδώ που να είναι multimodal.\n",
            "[18:08.080 --> 18:12.100]  Visual document, question and answering, video text to text.\n",
            "[18:13.780 --> 18:16.180]  Ναι, θα πρέπει λίγο να το ψάξετε.\n",
            "[18:17.860 --> 18:18.420]  Οκ.\n",
            "[18:18.520 --> 18:22.660]  Αλλά και κατά κύριο λόγο μπορείτε να φτιάξετε αρχικά τρία μεμονωμένα.\n",
            "[18:23.660 --> 18:29.220]  Με, πώς το λένε, με αυτό που είπαμε, με τα tasks εδώ στο text classification.\n",
            "[18:29.900 --> 18:30.200]  Ωραία.\n",
            "[18:30.200 --> 18:30.240]  Ωραία.\n",
            "[18:31.960 --> 18:34.840]  Έχουμε δείξει πώς κάνουμε fine-tune στην τάξη.\n",
            "[18:36.680 --> 18:40.540]  Επομένως, μπορείτε να χρησιμοποιήσετε πλέον και το GPT προφανώς.\n",
            "[18:41.320 --> 18:43.420]  Να κάνετε fine-tune τα μοντέλα σας.\n",
            "[18:44.280 --> 18:48.160]  Και μετά να τα χρησιμοποιήσετε σαν tools σε έναν AI agent.\n",
            "[18:48.160 --> 18:50.460]  Για να σας βγάζει πίσω μία απάντηση.\n",
            "[18:53.020 --> 18:57.220]  Ποιο πιστεύετε ότι θα είναι το μεγαλύτερο challenge, ας πούμε, του project.\n",
            "[18:59.220 --> 19:00.600]  Δεν νομίζω ότι θα έχετε κάτι.\n",
            "[19:01.340 --> 19:04.620]  Δοδεμένοι ότι είστε τέσσερις, δεν νομίζω ότι θα έχετε ιδιότερα κολλήματα.\n",
            "[19:04.760 --> 19:06.360]  Ίσως μόνο το training.\n",
            "[19:06.660 --> 19:07.460]  Ανεβάστε το.\n",
            "[19:08.180 --> 19:09.300]  Έχει κανείς GPU.\n",
            "[19:09.940 --> 19:10.680]  Ναι, έχω εγώ, ναι.\n",
            "[19:11.580 --> 19:12.600]  Τι GPU έχεις.\n",
            "[19:13.020 --> 19:13.780]  4090.\n",
            "[19:15.280 --> 19:16.260]  Εντάξει, καλή είναι.\n",
            "[19:18.160 --> 19:22.000]  Οπότε, τι θα αναλάβεις το βάρος στις εκπαίδευσεις.\n",
            "[19:23.020 --> 19:27.800]  Δεν μπορούμε με API key στο Hugging Face ή κάπου αλλού.\n",
            "[19:28.760 --> 19:30.720]  Εντάξει, την έχω, αλλά...\n",
            "[19:30.720 --> 19:34.860]  Και στο τέτοιο και στο Colab μπορείτε να κάνετε training.\n",
            "[19:37.300 --> 19:41.740]  Απλά στο Colab θα ανεβείτε πάνω και θα επιλέξετε GPU runtime Empire, νομίζετε, έτσι.\n",
            "[19:42.240 --> 19:42.540]  Ναι.\n",
            "[19:43.680 --> 19:44.040]  Αυτό.\n",
            "[19:45.180 --> 19:46.940]  Εντάξει, τα data δεν είναι πάρα πολλά.\n",
            "[19:47.160 --> 19:48.140]  20.000 είναι όλα.\n",
            "[19:48.160 --> 19:48.420]  Όλα, όλα.\n",
            "[19:51.240 --> 19:55.860]  Αλλά παίζουν ρόλο οι παράμετροι του μοντέλ που θα διαλέξουμε, φαντάζομαι.\n",
            "[19:56.340 --> 19:56.760]  Ορίστε.\n",
            "[19:57.280 --> 20:02.340]  Θα παίζουν ρόλο και οι παράμετροι του μοντέλ που θα διαλέξουμε για το πόσο βαρύ θα είναι και αν μπορεί να τρέξει.\n",
            "[20:02.660 --> 20:04.000]  Προφανώς, προφανώς.\n",
            "[20:04.860 --> 20:05.380]  Προφανώς.\n",
            "[20:06.340 --> 20:13.980]  Εγώ θα σας έλεγα, φτιάχτε για κάθε μοντέλο μέσω κέρας ένα ελαφρύ μοντέλο να κάνετε train, να δείτε πώς θα πάτε.\n",
            "[20:13.980 --> 20:17.560]  Μετά κάνετε fine tune ένα pre-trained model.\n",
            "[20:18.160 --> 20:20.220]  Από το hacking phase.\n",
            "[20:21.380 --> 20:24.120]  Θα δείτε προφανώς ότι το δεύτερο πάει καλύτερα.\n",
            "[20:25.460 --> 20:28.540]  Αλλά αυτό είναι, λοιπόν, να το ψάξετε η όλη φάση.\n",
            "[20:29.160 --> 20:36.180]  Και αφού γίνει αυτό, τα κουμπών σαν tools θα είναι δεχομένως ένα AI agent, άμα έχετε χρόνο.\n",
            "[20:36.820 --> 20:38.220]  Και έχετε ένα end-to-end σύστημα.\n",
            "[20:39.680 --> 20:39.880]  Οκ.\n",
            "[20:42.000 --> 20:42.440]  Εσύ.\n",
            "[20:43.180 --> 20:43.820]  Αυτά.\n",
            "[20:48.160 --> 20:54.100]  Ναι.\n",
            "[21:00.380 --> 21:00.900]  Ναι.\n",
            "[21:02.900 --> 21:03.420]  Ναι.\n",
            "[21:05.760 --> 21:08.160]  Εντάξει, θα ψάξουμε να δούμε και κάτι καλύτερο, αλλά...\n",
            "[21:09.700 --> 21:11.740]  Οκ. Ωραία. Θα πάμε καλύτερα.\n",
            "[21:12.180 --> 21:14.160]  Και θα σας πούμε στην τάξη αν έχουμε κάποιο...\n",
            "[21:15.460 --> 21:16.280]  Ωραία, ωραία.\n",
            "[21:16.280 --> 21:16.340]  Ωραία.\n",
            "[21:16.540 --> 21:16.900]  Αλλαγή.\n",
            "[21:17.280 --> 21:18.040]  Από τις εταιρείες...\n",
            "[21:18.040 --> 21:18.140]  Από τις εταιρείες...\n",
            "[21:18.140 --> 21:20.360]  Στις εταιρείες σας δεν έχει κανένας δεδομένα τίποτα, μπιτ.\n",
            "[21:23.600 --> 21:25.160]  Σε τι εταιρείες βλέπετε.\n",
            "[21:27.200 --> 21:28.140]  Να, εδώ, στην Geyser.\n",
            "[21:29.720 --> 21:31.420]  Έχουμε τζίρα, εμείς, τεκέτη.\n",
            "[21:33.540 --> 21:34.140]  Αλλά τι...\n",
            "[21:35.100 --> 21:39.140]  Πάντως, αν παίρνετε δεδομένα, ξέρετε ότι αν παίρνετε δεδομένα από την Geyser, εμπιχεί,\n",
            "[21:40.080 --> 21:43.940]  για να το φτιάξετε και να το δοκιμάσετε και να παίξετε και τα λοιπά,\n",
            "[21:44.720 --> 21:48.120]  εμείς δεν θα σας ζητήσουμε το δεδομένα όταν πρόκειται για τέτοιου είδους δεδομένα.\n",
            "[21:48.140 --> 21:53.140]  Απλά να μας δείξετε, ρε παιδί μου, ένα snapshot ότι είναι κάπως έτσι.\n",
            "[21:53.980 --> 21:56.800]  Για να καταλάβουμε και εμείς τι διαβάζουμε όταν διαβάζουμε τον κώδικα.\n",
            "[21:58.440 --> 21:58.780]  Αυτό.\n",
            "[22:00.240 --> 22:05.060]  Επομένως, μπορείτε να χρησιμοποιήσετε data από την Geyser, αν το θέλουνε,\n",
            "[22:05.520 --> 22:07.520]  για να το κάνετε train και τα λοιπά και τα λοιπά.\n",
            "[22:09.340 --> 22:10.260]  Από τζίρα είναι.\n",
            "[22:10.860 --> 22:11.740]  Από τζίρα, ναι.\n",
            "[22:11.740 --> 22:13.740]  Και έχουν τέτοια...\n",
            "[22:14.300 --> 22:18.060]  Εκεί έχουν κάποια classifications για topics, για sentiment,\n",
            "[22:18.140 --> 22:19.260]  για urgency, τέτοια.\n",
            "[22:19.800 --> 22:20.600]  Έχουν, ναι.\n",
            "[22:21.020 --> 22:25.300]  Για agency σίγουρα και για τοπι πρέπει να έχουμε.\n",
            "[22:26.200 --> 22:28.320]  Για sentiment δεν νομίζω ότι έχει.\n",
            "[22:28.980 --> 22:29.640]  Sentiment όχι.\n",
            "[22:31.720 --> 22:32.100]  Οκ.\n",
            "[22:33.880 --> 22:37.860]  Κοίτατε ότι το sentiment θα μπορούσατε να το δώσετε να το κάνει και το LLM.\n",
            "[22:39.860 --> 22:40.900]  Λέω εγώ τώρα.\n",
            "[22:42.340 --> 22:42.640]  Οκ.\n",
            "[22:42.640 --> 22:45.180]  Να του λέτε ότι γύρισε μου το sentiment\n",
            "[22:45.180 --> 22:48.100]  και μάλιστα μπορεί να κάνεις και aspect-based.\n",
            "[22:48.140 --> 22:53.500]  Αν ξέρετε τι είναι το ABSA, έτσι λέγεται.\n",
            "[22:53.500 --> 22:56.200]  ABSA, aspect-based sentiment analysis.\n",
            "[22:56.200 --> 22:59.300]  Όταν έχεις ξεκινάει review στους Scrooge για ένα κινητό,\n",
            "[22:59.300 --> 23:02.000]  κάποιος μπορεί να δώσει μια overall κριτική,\n",
            "[23:02.000 --> 23:03.340]  αλλά μπορεί να σου πει ότι έχει καλή μνήμη,\n",
            "[23:03.340 --> 23:05.800]  αλλά καπλαστικά ή να έχει κακή οθόνη.\n",
            "[23:06.520 --> 23:06.900]  Οκ.\n",
            "[23:06.900 --> 23:11.240]  Οπότε εκεί το τελικό sentiment δεν μπορείτε να το βγάλετε.\n",
            "[23:11.940 --> 23:14.640]  Είναι per aspect, per specification.\n",
            "[23:15.280 --> 23:15.580]  Ναι.\n",
            "[23:15.580 --> 23:18.140]  Αυτό κάνει το ABSA ουσιαστικά.\n",
            "[23:18.140 --> 23:20.140]  Αλλά και αυτό με ένα απλό.\n",
            "[23:20.140 --> 23:25.140]  Μπορείτε να το ρίξετε στο charge.gpt και να σας φτιάξει ένα prompt\n",
            "[23:25.140 --> 23:30.140]  για το πώς θα μπορούσατε να ρωτήσετε σε έναν agent\n",
            "[23:30.140 --> 23:34.140]  για να σου κάνει aspect-based sentiment analysis\n",
            "[23:34.140 --> 23:36.140]  και πώς θα το γυρίσετε πίσω.\n",
            "[23:36.140 --> 23:41.140]  Να σου δίνει δηλαδή το aspect, το sentiment και ένα σκορ ενδεχομένως.\n",
            "[23:41.140 --> 23:43.140]  Κάπως έτσι.\n",
            "[23:43.140 --> 23:47.140]  Εγώ θα σας έλεγα προσπαθήστε να πάρετε real data, παιδιά, από τις εταιρείες σας.\n",
            "[23:48.140 --> 23:52.140]  Είναι πολύ πιο worth αυτό το πράγμα.\n",
            "[23:52.140 --> 23:55.140]  Παρά να πάρετε ένα ψεύτικο data set.\n",
            "[23:55.140 --> 23:57.140]  My personal opinion, έτσι.\n",
            "[23:57.140 --> 24:00.140]  Γιατί είναι και πιο engaging.\n",
            "[24:00.140 --> 24:02.140]  Αυτό.\n",
            "[24:02.140 --> 24:06.140]  Δεν ξέρω κατά πόσο είναι ανοιχτήστο να δώσουμε αυτό.\n",
            "[24:06.140 --> 24:08.140]  Καλά, και τείχους έχουμε πάει.\n",
            "[24:10.140 --> 24:13.140]  Παρ' όλα αυτά, οκ. Θα προσπαθήσουμε.\n",
            "[24:13.140 --> 24:15.140]  Ναι, κάντε μια ερώτηση.\n",
            "[24:15.140 --> 24:16.140]  Κάντε μια ερώτηση.\n",
            "[24:16.140 --> 24:17.140]  Ναι.\n",
            "[24:17.140 --> 24:19.140]  Αυτό.\n",
            "[24:19.140 --> 24:21.140]  Ωραία.\n",
            "[24:21.140 --> 24:23.140]  Ευχαριστούμε πολύ.\n",
            "[24:39.140 --> 24:41.140]  Παιδιά, κανονικά είναι τέλος Ιουλίου.\n",
            "[24:41.140 --> 24:43.140]  Οκ.\n",
            "[24:43.140 --> 24:45.140]  Εσείς ζητάτε και πάει τέλος Αυγούστου.\n",
            "[24:45.140 --> 24:49.140]  Και μετά έρχεστε 30 Αυγούστου και μας ζητάτε και άλλες δύο εβδομάδες.\n",
            "[24:49.140 --> 24:51.140]  Επομένως,\n",
            "[24:51.140 --> 24:53.140]  not to you.\n",
            "[24:53.140 --> 24:58.140]  Οπότε, όσο νωρίτερα παραδώσετε, τόσο νωρίτερα θα εξεταστείτε.\n",
            "[25:08.140 --> 25:12.140]  Ε, συνήθως, παίρνετε άλλη μια εβδομάδα, max δύο, το Σεπτέμβριο.\n",
            "[25:12.140 --> 25:13.140]  Εντάξει.\n",
            "[25:13.140 --> 25:14.140]  Εντάξει.\n",
            "[25:14.140 --> 25:21.140]  Και τα deliverables πρέπει να είναι ο κώδικας, ένα report και ένα presentation.\n",
            "[25:21.140 --> 25:24.140]  Ο κώδικας, το report, τα βάρη, τα μοντέλα.\n",
            "[25:24.140 --> 25:29.140]  Το presentation θα μας το στείλετε μια μέρα πριν την τελική παρουσίαση.\n",
            "[25:29.140 --> 25:31.140]  Δηλαδή, το PPT δεν το χρειαζόμαστε άμεσα.\n",
            "[25:31.140 --> 25:33.140]  Το report χρειαζόμαστε.\n",
            "[25:33.140 --> 25:37.140]  Αλλά έχουμε βάλει πάνω το πώς θέλουμε να είναι το report, έτσι.\n",
            "[25:37.140 --> 25:40.140]  Έχετε βγει να το δείτε ως κριτός, μπράβο.\n",
            "[25:40.140 --> 25:43.140]  Και μην αφήσετε το business απ' έξω.\n",
            "[25:44.140 --> 25:45.140]  Εντάξει.\n",
            "[25:45.140 --> 25:46.140]  Ναι, ναι.\n",
            "[25:46.140 --> 25:50.140]  Είναι 30% του βαθμού. Το έχω ξαναπεί. Έχει μαγειάσει η γλώσσα μου.\n",
            "[25:50.140 --> 25:52.140]  Οκ.\n",
            "[25:52.140 --> 25:54.140]  Ναι.\n",
            "[25:54.140 --> 25:56.140]  Θέλει συνέχεια.\n",
            "[25:56.140 --> 25:58.140]  Ευχαριστούμε για τα χρόνια σας.\n",
            "[25:58.140 --> 26:00.140]  Να είστε καλά, παιδιά. Γεια χαρά.\n",
            "[26:00.140 --> 26:02.140]  Γεια σας. Καλό βήμα.\n"
          ]
        }
      ]
    }
  ]
}