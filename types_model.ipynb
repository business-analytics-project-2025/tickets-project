{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fade76c2-a676-4d87-a7a1-205820b3c321",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/isarris/anaconda3/lib/python3.11/site-packages (4.32.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/isarris/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: filelock in /Users/isarris/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/isarris/anaconda3/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/isarris/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers scikit-learn\n",
    "\n",
    "import os, json, random, numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c74cdf3-0bfa-4118-891d-a666c7a59cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "SEED = 42\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "DATA_PATH  = \"Cleaned_Tickets.csv\"\n",
    "MAX_LEN    = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS     = 4\n",
    "LR_HEAD    = 2e-4\n",
    "PATIENCE   = 1\n",
    "SAVE_DIR   = \"./type_base_model_pt\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe96946-b0fe-4650-8657-7740f60fc2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Data\n",
    "# -----------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[\"subject\"] = df[\"subject\"].fillna(\"\")\n",
    "df[\"body\"]    = df[\"body\"].fillna(\"\")\n",
    "df[\"text\"]    = (df[\"subject\"] + \" \" + df[\"body\"]).str.strip()\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"type\"])\n",
    "problem_idx = np.where(le.classes_ == \"Problem\")[0][0]  # index of Problem class\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df[[\"text\",\"label\"]],\n",
    "    test_size=0.2, random_state=SEED, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class TicketDS(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True, max_length=self.max_len, padding=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = TicketDS(train_df[\"text\"], train_df[\"label\"], tokenizer, MAX_LEN)\n",
    "val_ds   = TicketDS(val_df[\"text\"],   val_df[\"label\"],   tokenizer, MAX_LEN)\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collator)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c502d45-c49b-4f8e-a297-15c9292c35bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Model: DistilBERT (frozen) + linear head, MEAN POOLING\n",
    "# -----------------------\n",
    "class DistilBertClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout=0.2, freeze_encoder=True):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        if freeze_encoder:\n",
    "            for p in self.bert.parameters():\n",
    "                p.requires_grad = False\n",
    "        hidden = self.bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden = out.last_hidden_state                 # [B, T, H]\n",
    "        mask = attention_mask.unsqueeze(-1)            # [B, T, 1]\n",
    "        summed = (hidden * mask).sum(dim=1)            # [B, H]\n",
    "        counts = mask.sum(dim=1).clamp(min=1e-9)       # [B, 1]\n",
    "        mean_pooled = summed / counts                  # [B, H]\n",
    "        x = self.dropout(mean_pooled)\n",
    "        return self.classifier(x)\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "model = DistilBertClassifier(MODEL_NAME, num_labels, freeze_encoder=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "473a00a2-6e2a-412f-a8bd-4bbfb5c7449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Class-weighted loss (softened + Problem boost)\n",
    "# -----------------------\n",
    "train_labels_np = train_df[\"label\"].to_numpy()\n",
    "raw_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.arange(num_labels),\n",
    "    y=train_labels_np\n",
    ").astype(np.float32)\n",
    "soft_weights = 0.7 * raw_weights + 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ed20532-06ba-475e-b877-672e827b5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boost Problem class by +15%\n",
    "soft_weights[problem_idx] *= 1.15\n",
    "\n",
    "weights_tensor = torch.tensor(soft_weights, dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_HEAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b12e1beb-0746-4d89-bc8b-8e45d2279e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Train / Eval helpers\n",
    "# -----------------------\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    losses, preds_all, labels_all = [], [], []\n",
    "    for batch in tqdm(loader, leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "        preds_all.append(preds)\n",
    "        labels_all.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    preds_all = np.concatenate(preds_all)\n",
    "    labels_all = np.concatenate(labels_all)\n",
    "    f1w = f1_score(labels_all, preds_all, average=\"weighted\")\n",
    "    return np.mean(losses), f1w, preds_all, labels_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87d2336c-dbea-4b8d-b122-6dc2552fffa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff9632c4d884b58afe9b1742216d5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143e5ba91535403babb084d5dec2c1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frozen+MeanPool+ProblemBoost] Epoch 1/4 | train loss 1.0532 f1w 0.6317 | val loss 0.8431 f1w 0.6964\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80d7a7ea213498eb61c9c2d5f9b04d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b237dc0c06a14ff89aeb5a0bc337f4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frozen+MeanPool+ProblemBoost] Epoch 2/4 | train loss 0.7733 f1w 0.7119 | val loss 0.6925 f1w 0.7301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e96de9967244febf76a10733a78515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4ecb8246844066bfc4f7b528b892c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frozen+MeanPool+ProblemBoost] Epoch 3/4 | train loss 0.6680 f1w 0.7365 | val loss 0.6172 f1w 0.7438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ff85c776144cbba4a6248725e06e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b38bb1f19674d9fb6dde5144f50cc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frozen+MeanPool+ProblemBoost] Epoch 4/4 | train loss 0.6114 f1w 0.7447 | val loss 0.5715 f1w 0.7574\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Training loop (frozen only)\n",
    "# -----------------------\n",
    "best_f1, best_state, patience = -1.0, None, PATIENCE\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_f1, _ , _  = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_f1, va_pred, va_true = run_epoch(val_loader, train=False)\n",
    "    print(f\"[Frozen+MeanPool+ProblemBoost] Epoch {epoch}/{EPOCHS} | train loss {tr_loss:.4f} f1w {tr_f1:.4f} | val loss {va_loss:.4f} f1w {va_f1:.4f}\")\n",
    "\n",
    "    if va_f1 > best_f1:\n",
    "        best_f1 = va_f1\n",
    "        best_state = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"label_classes\": le.classes_,\n",
    "            \"tokenizer_name\": MODEL_NAME,\n",
    "            \"max_len\": MAX_LEN\n",
    "        }\n",
    "        patience = PATIENCE\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience < 0:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5dea1b13-8b7a-4a89-9e47-1229bec61063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model weights + tokenizer + metadata to: ./type_base_model_pt\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# SAVE — tensors only + JSON metadata\n",
    "# -----------------------\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "torch.save(best_state[\"model\"], os.path.join(SAVE_DIR, \"base_model_weights.pt\"))\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "meta = {\n",
    "    \"label_classes\": best_state[\"label_classes\"].tolist(),\n",
    "    \"tokenizer_name\": best_state[\"tokenizer_name\"],\n",
    "    \"max_len\": int(best_state[\"max_len\"])\n",
    "}\n",
    "with open(os.path.join(SAVE_DIR, \"metadata.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Saved model weights + tokenizer + metadata to:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a4bdadb-b480-4e1b-8b7a-3134fd4c178d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6cb0addf224071a6d9885d7624e52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Change       0.81      0.93      0.87       341\n",
      "    Incident       0.73      0.74      0.74      1314\n",
      "     Problem       0.49      0.46      0.47       680\n",
      "     Request       0.97      0.94      0.95       933\n",
      "\n",
      "    accuracy                           0.76      3268\n",
      "   macro avg       0.75      0.77      0.76      3268\n",
      "weighted avg       0.76      0.76      0.76      3268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Final report (reload + evaluate)\n",
    "# -----------------------\n",
    "state_dict = torch.load(os.path.join(SAVE_DIR, \"model_state_dict.pt\"), map_location=device)\n",
    "reloaded = DistilBertClassifier(meta[\"tokenizer_name\"], num_labels=len(meta[\"label_classes\"]), freeze_encoder=True).to(device)\n",
    "reloaded.load_state_dict(state_dict)\n",
    "reloaded.eval()\n",
    "\n",
    "def eval_with(model_to_eval):\n",
    "    _, _, va_pred, va_true = run_epoch(val_loader, train=False)\n",
    "    print(classification_report(va_true, va_pred, target_names=le.classes_))\n",
    "\n",
    "eval_with(reloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c6d3863-5543-44c7-8ac8-8615a6a6aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Loader + inference helpers\n",
    "# -----------------------\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_type_model(save_dir=SAVE_DIR, device=device):\n",
    "    with open(os.path.join(save_dir, \"metadata.json\")) as f:\n",
    "        meta = json.load(f)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "    model = DistilBertClassifier(meta[\"tokenizer_name\"], num_labels=len(meta[\"label_classes\"]), freeze_encoder=True)\n",
    "    state_dict = torch.load(os.path.join(save_dir, \"model_state_dict.pt\"), map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device).eval()\n",
    "    le_loaded = LabelEncoder(); le_loaded.fit(meta[\"label_classes\"])\n",
    "    return model, tokenizer, le_loaded, meta\n",
    "\n",
    "def predict_types(texts, model, tokenizer, label_encoder, max_len, device=device):\n",
    "    enc = tokenizer(texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "    idx = probs.argmax(axis=1)\n",
    "    return label_encoder.inverse_transform(idx), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4d3b494-2c2c-4013-ba9a-22f849abd26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Problem' 'Problem' 'Incident']\n"
     ]
    }
   ],
   "source": [
    "# Smoke test\n",
    "mdl, tok, le_loaded, meta_loaded = load_type_model(SAVE_DIR, device=device)\n",
    "labels, _ = predict_types(\n",
    "    [\"Please reset my password\", \"Payments page throws 500 error\", \"Planned maintenance tonight 22:00\"],\n",
    "    mdl, tok, le_loaded, meta_loaded[\"max_len\"], device=device\n",
    ")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16952443-c931-4c10-a1b8-cd381b6eafb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73698f9607954a4db1d384cbcfbc3de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Run evaluation to get predictions\n",
    "_, _, va_pred, va_true = run_epoch(val_loader, train=False)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(va_true, va_pred)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=le.classes_,\n",
    "            yticklabels=le.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix — TYPE classification\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
