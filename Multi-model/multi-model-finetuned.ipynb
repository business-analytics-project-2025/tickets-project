{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a25c7f5-8cfa-42b0-a7b2-6ef7246df561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0688a342628d483991237dc39771f830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6b97ac471f40b1bb3600a16a3d3e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FineTune] Epoch 1 | train loss 3.1788 | val loss 3.2375\n",
      "   F1(type)=0.760 | F1(dept)=0.293 | F1(prio)=0.423 | F1(tags)=0.552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdde31f8184d4e0dae8f1b59a9ac8807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84f02929e5d49119cb9b1a75d944cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FineTune] Epoch 2 | train loss 3.0849 | val loss 3.1667\n",
      "   F1(type)=0.809 | F1(dept)=0.305 | F1(prio)=0.430 | F1(tags)=0.545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d48160b3b62462fbcb6038aca8b93a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc4a8c002b44bb88b0a31d1018aa2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FineTune] Epoch 3 | train loss 3.0154 | val loss 3.1554\n",
      "   F1(type)=0.821 | F1(dept)=0.291 | F1(prio)=0.444 | F1(tags)=0.553\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc657f5d305484598fb4557f6bd3884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0c9fcfc91442e9a3419270f7049fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FineTune] Epoch 4 | train loss 2.9479 | val loss 3.1083\n",
      "   F1(type)=0.825 | F1(dept)=0.306 | F1(prio)=0.437 | F1(tags)=0.551\n",
      "Fine-tuning complete. Best avg F1: 0.5297076522309223\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Stage 2: Fine-tuning Encoder\n",
    "# ============================\n",
    "\n",
    "# 1) Imports\n",
    "import os, random, numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Config\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "DATA_PATH  = \"Cleaned_Tickets.csv\"\n",
    "MAX_LEN    = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_FINE = 4\n",
    "PATIENCE   = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Reload dataset\n",
    "# ----------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[\"subject\"] = df[\"subject\"].fillna(\"\")\n",
    "df[\"body\"]    = df[\"body\"].fillna(\"\")\n",
    "df[\"text\"]    = \"[SUBJ] \" + df[\"subject\"] + \" [BODY] \" + df[\"body\"]\n",
    "\n",
    "type_classes = sorted(df[\"type\"].unique())\n",
    "dept_classes = sorted(df[\"department\"].unique())\n",
    "prio_classes = sorted(df[\"priority\"].unique())\n",
    "type2id = {c:i for i,c in enumerate(type_classes)}\n",
    "dept2id = {c:i for i,c in enumerate(dept_classes)}\n",
    "prio2id = {c:i for i,c in enumerate(prio_classes)}\n",
    "\n",
    "tag_cols = [c for c in df.columns if c.startswith(\"tag_\")]\n",
    "df[\"tags_list\"] = df[tag_cols].fillna(\"\").values.tolist()\n",
    "df[\"tags_list\"] = df[\"tags_list\"].apply(lambda tags: [t for t in tags if t != \"\"])\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(df[\"tags_list\"])\n",
    "tag_classes = mlb.classes_\n",
    "\n",
    "# split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df[\"type\"])\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# dataset\n",
    "class TicketDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, type2id, dept2id, prio2id, mlb):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.type_labels = df[\"type\"].map(type2id).tolist()\n",
    "        self.dept_labels = df[\"department\"].map(dept2id).tolist()\n",
    "        self.prio_labels = df[\"priority\"].map(prio2id).tolist()\n",
    "        self.tags_labels = mlb.transform(df[\"tags_list\"]).astype(np.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx], truncation=True, padding=\"max_length\",\n",
    "            max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"y_type\": torch.tensor(self.type_labels[idx], dtype=torch.long),\n",
    "            \"y_dept\": torch.tensor(self.dept_labels[idx], dtype=torch.long),\n",
    "            \"y_prio\": torch.tensor(self.prio_labels[idx], dtype=torch.long),\n",
    "            \"y_tags\": torch.tensor(self.tags_labels[idx], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "train_loader = DataLoader(TicketDataset(train_df, tokenizer, MAX_LEN, type2id, dept2id, prio2id, mlb),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(TicketDataset(val_df, tokenizer, MAX_LEN, type2id, dept2id, prio2id, mlb),\n",
    "                          batch_size=BATCH_SIZE)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Model\n",
    "# ----------------------------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, p=0.2):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "        )\n",
    "    def forward(self, x): return self.seq(x)\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME, dropout=0.2, freeze_encoder=False,\n",
    "                 n_type=len(type_classes), n_dept=len(dept_classes),\n",
    "                 n_prio=len(prio_classes), n_tags=len(tag_classes)):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        if freeze_encoder:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.type_head = Head(hidden, n_type, p=0.2)\n",
    "        self.dept_head = Head(hidden, n_dept, p=0.2)\n",
    "        self.prio_head = Head(hidden, n_prio, p=0.2)\n",
    "        self.tags_head = Head(hidden, n_tags, p=0.2)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden = out.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).expand(hidden.size()).float()\n",
    "        summed = torch.sum(hidden * mask, 1)\n",
    "        counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "        pooled = summed / counts\n",
    "        pooled = self.dropout(pooled)\n",
    "        return {\n",
    "            \"type\": self.type_head(pooled),\n",
    "            \"department\": self.dept_head(pooled),\n",
    "            \"priority\": self.prio_head(pooled),\n",
    "            \"tags\": self.tags_head(pooled)\n",
    "        }\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Load Stage 1 checkpoint\n",
    "# ----------------------------\n",
    "checkpoint = torch.load(\"multitask_best.pt\", map_location=device, weights_only=False)\n",
    "\n",
    "model = MultiTaskModel(\n",
    "    freeze_encoder=False,\n",
    "    n_type=len(type_classes), n_dept=len(dept_classes),\n",
    "    n_prio=len(prio_classes), n_tags=len(tag_classes)\n",
    ").to(device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Unfreeze top encoder layers\n",
    "# ----------------------------\n",
    "for name, p in model.encoder.named_parameters():\n",
    "    if \"layer.5\" in name or \"layer.4\" in name:\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Loss functions\n",
    "# ----------------------------\n",
    "weights_type = compute_class_weight(\"balanced\", classes=np.arange(len(type_classes)), y=train_df[\"type\"].map(type2id))\n",
    "weights_dept = compute_class_weight(\"balanced\", classes=np.arange(len(dept_classes)), y=train_df[\"department\"].map(dept2id))\n",
    "weights_prio = compute_class_weight(\"balanced\", classes=np.arange(len(prio_classes)), y=train_df[\"priority\"].map(prio2id))\n",
    "\n",
    "criterion_type = nn.CrossEntropyLoss(weight=torch.tensor(weights_type, dtype=torch.float32).to(device))\n",
    "criterion_dept = nn.CrossEntropyLoss(weight=torch.tensor(weights_dept, dtype=torch.float32).to(device))\n",
    "criterion_prio = nn.CrossEntropyLoss(weight=torch.tensor(weights_prio, dtype=torch.float32).to(device))\n",
    "\n",
    "train_tags = mlb.transform(train_df[\"tags_list\"])\n",
    "pos = train_tags.sum(axis=0); neg = train_tags.shape[0] - pos\n",
    "pos_weight_raw = (neg / np.maximum(pos, 1))\n",
    "pos_weight = np.clip(pos_weight_raw, 1.0, 10.0)\n",
    "criterion_tags = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, dtype=torch.float32).to(device))\n",
    "\n",
    "ALPHA, BETA, GAMMA, DELTA = 1.0, 1.0, 1.0, 1.2\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Optimizer\n",
    "# ----------------------------\n",
    "encoder_params = [p for n,p in model.named_parameters() if p.requires_grad and \"encoder\" in n]\n",
    "head_params    = [p for n,p in model.named_parameters() if p.requires_grad and \"encoder\" not in n]\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": encoder_params, \"lr\": 5e-6},\n",
    "    {\"params\": head_params, \"lr\": 3e-5},\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Training loop\n",
    "# ----------------------------\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    losses, preds, labels = [], {\"type\":[], \"department\":[], \"priority\":[], \"tags\":[]}, {\"type\":[], \"department\":[], \"priority\":[], \"tags\":[]}\n",
    "    for batch in tqdm(loader, leave=False):\n",
    "        input_ids, attn = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device)\n",
    "        y_type, y_dept, y_prio, y_tags = batch[\"y_type\"].to(device), batch[\"y_dept\"].to(device), batch[\"y_prio\"].to(device), batch[\"y_tags\"].to(device)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            out = model(input_ids, attn)\n",
    "            loss = (ALPHA*criterion_type(out[\"type\"], y_type) +\n",
    "                    BETA*criterion_dept(out[\"department\"], y_dept) +\n",
    "                    GAMMA*criterion_prio(out[\"priority\"], y_prio) +\n",
    "                    DELTA*criterion_tags(out[\"tags\"], y_tags))\n",
    "            if train:\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        preds[\"type\"].append(torch.argmax(out[\"type\"],1).detach().cpu().numpy()); labels[\"type\"].append(y_type.cpu().numpy())\n",
    "        preds[\"department\"].append(torch.argmax(out[\"department\"],1).detach().cpu().numpy()); labels[\"department\"].append(y_dept.cpu().numpy())\n",
    "        preds[\"priority\"].append(torch.argmax(out[\"priority\"],1).detach().cpu().numpy()); labels[\"priority\"].append(y_prio.cpu().numpy())\n",
    "        preds[\"tags\"].append(torch.sigmoid(out[\"tags\"]).detach().cpu().numpy()); labels[\"tags\"].append(y_tags.cpu().numpy())\n",
    "    for k in preds: preds[k] = np.concatenate(preds[k]); labels[k] = np.concatenate(labels[k])\n",
    "    return np.mean(losses), preds, labels\n",
    "\n",
    "best_f1, patience = -1, PATIENCE\n",
    "for epoch in range(1, EPOCHS_FINE+1):\n",
    "    tr_loss, tr_preds, tr_labels = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_preds, va_labels = run_epoch(val_loader, train=False)\n",
    "\n",
    "    f1_type = f1_score(va_labels[\"type\"], va_preds[\"type\"], average=\"macro\")\n",
    "    f1_dept = f1_score(va_labels[\"department\"], va_preds[\"department\"], average=\"macro\")\n",
    "    f1_prio = f1_score(va_labels[\"priority\"], va_preds[\"priority\"], average=\"macro\")\n",
    "    va_bin = (va_preds[\"tags\"] >= checkpoint[\"tag_thresholds\"]).astype(int)\n",
    "    f1_tags = f1_score(va_labels[\"tags\"], va_bin, average=\"micro\")\n",
    "\n",
    "    print(f\"[FineTune] Epoch {epoch} | train loss {tr_loss:.4f} | val loss {va_loss:.4f}\")\n",
    "    print(f\"   F1(type)={f1_type:.3f} | F1(dept)={f1_dept:.3f} | F1(prio)={f1_prio:.3f} | F1(tags)={f1_tags:.3f}\")\n",
    "\n",
    "    avg_f1 = (f1_type+f1_dept+f1_prio+f1_tags)/4\n",
    "    if avg_f1 > best_f1:\n",
    "        best_f1 = avg_f1\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"tokenizer_name\": MODEL_NAME,\n",
    "            \"max_len\": MAX_LEN,\n",
    "            \"type_classes\": type_classes,\n",
    "            \"dept_classes\": dept_classes,\n",
    "            \"prio_classes\": prio_classes,\n",
    "            \"tag_classes\": list(tag_classes),\n",
    "            \"tag_thresholds\": checkpoint[\"tag_thresholds\"]\n",
    "        }, \"multitask_finetuned.pt\")\n",
    "        patience = PATIENCE\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience < 0:\n",
    "            print(\"Early stopping.\"); break\n",
    "\n",
    "print(\"Fine-tuning complete. Best avg F1:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9216906a-9798-4a5b-9f74-ec7c2ef33e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86369c0dca2a407d836ca5698b805d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TYPE REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Change       0.95      1.00      0.97       341\n",
      "    Incident       0.79      0.74      0.76      1314\n",
      "     Problem       0.55      0.61      0.58       680\n",
      "     Request       1.00      0.98      0.99       933\n",
      "\n",
      "    accuracy                           0.81      3268\n",
      "   macro avg       0.82      0.83      0.83      3268\n",
      "weighted avg       0.81      0.81      0.81      3268\n",
      "\n",
      "\n",
      "=== DEPARTMENT REPORT ===\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "           Billing And Payments       0.62      0.75      0.68       327\n",
      "               Customer Service       0.23      0.18      0.20       434\n",
      "                General Inquiry       0.08      0.37      0.14        51\n",
      "                Human Resources       0.19      0.28      0.22        76\n",
      "                     It Support       0.23      0.24      0.24       345\n",
      "                Product Support       0.35      0.16      0.22       644\n",
      "          Returns And Exchanges       0.17      0.41      0.24       170\n",
      "            Sales And Pre-Sales       0.15      0.45      0.22        96\n",
      "Service Outages And Maintenance       0.39      0.77      0.52       144\n",
      "              Technical Support       0.52      0.30      0.38       981\n",
      "\n",
      "                       accuracy                           0.33      3268\n",
      "                      macro avg       0.29      0.39      0.31      3268\n",
      "                   weighted avg       0.38      0.33      0.33      3268\n",
      "\n",
      "\n",
      "=== PRIORITY REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.59      0.35      0.44      1232\n",
      "         Low       0.32      0.54      0.40       671\n",
      "      Medium       0.47      0.48      0.47      1365\n",
      "\n",
      "    accuracy                           0.44      3268\n",
      "   macro avg       0.46      0.46      0.44      3268\n",
      "weighted avg       0.49      0.44      0.45      3268\n",
      "\n",
      "\n",
      "=== TAGS REPORT ===\n",
      "F1-micro: 0.615 | F1-macro: 0.092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isarris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1757: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Final Evaluation (Validation)\n",
    "# ============================\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Reload fine-tuned checkpoint\n",
    "checkpoint_ft = torch.load(\"multitask_finetuned.pt\", map_location=device, weights_only=False)\n",
    "\n",
    "model = MultiTaskModel(\n",
    "    freeze_encoder=False,\n",
    "    n_type=len(checkpoint_ft[\"type_classes\"]),\n",
    "    n_dept=len(checkpoint_ft[\"dept_classes\"]),\n",
    "    n_prio=len(checkpoint_ft[\"prio_classes\"]),\n",
    "    n_tags=len(checkpoint_ft[\"tag_classes\"])\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint_ft[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "# Run inference on validation set\n",
    "_, va_preds, va_labels = run_epoch(val_loader, train=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Reports for type/dept/prio\n",
    "# ----------------------------\n",
    "print(\"\\n=== TYPE REPORT ===\")\n",
    "print(classification_report(\n",
    "    va_labels[\"type\"], va_preds[\"type\"],\n",
    "    target_names=checkpoint_ft[\"type_classes\"]\n",
    "))\n",
    "\n",
    "print(\"\\n=== DEPARTMENT REPORT ===\")\n",
    "print(classification_report(\n",
    "    va_labels[\"department\"], va_preds[\"department\"],\n",
    "    target_names=checkpoint_ft[\"dept_classes\"]\n",
    "))\n",
    "\n",
    "print(\"\\n=== PRIORITY REPORT ===\")\n",
    "print(classification_report(\n",
    "    va_labels[\"priority\"], va_preds[\"priority\"],\n",
    "    target_names=checkpoint_ft[\"prio_classes\"]\n",
    "))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Metrics for tags\n",
    "# ----------------------------\n",
    "def tune_tag_thresholds(Y_true, Y_prob, grid=np.linspace(0.1,0.9,9)):\n",
    "    T = Y_true.shape[1]\n",
    "    best = np.full(T, 0.5, dtype=np.float32)\n",
    "    for t in range(T):\n",
    "        yt = Y_true[:, t]\n",
    "        if yt.sum() == 0:  # skip tags not present\n",
    "            continue\n",
    "        yp = Y_prob[:, t]\n",
    "        best_f, best_thr = -1.0, 0.5\n",
    "        for thr in grid:\n",
    "            f1 = f1_score(yt, (yp >= thr).astype(int), zero_division=0)\n",
    "            if f1 > best_f:\n",
    "                best_f, best_thr = f1, thr\n",
    "        best[t] = best_thr\n",
    "    return best\n",
    "\n",
    "val_probs = va_preds[\"tags\"]\n",
    "val_true  = va_labels[\"tags\"]\n",
    "\n",
    "tag_thresholds = tune_tag_thresholds(val_true, val_probs)\n",
    "val_bin = (val_probs >= tag_thresholds).astype(int)\n",
    "\n",
    "f1_micro = f1_score(val_true, val_bin, average=\"micro\")\n",
    "f1_macro = f1_score(val_true, val_bin, average=\"macro\")\n",
    "\n",
    "print(\"\\n=== TAGS REPORT ===\")\n",
    "print(f\"F1-micro: {f1_micro:.3f} | F1-macro: {f1_macro:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
